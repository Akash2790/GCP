Here's an example Python script that uses the Google Cloud Storage REST API to list all the buckets in a project and check their IAM policies to ensure they are not publicly accessible:


------------------------------------------------------------------------------------------------------------------------------

import requests
import json

project_name = "<ProjectName>"
headers = {
    "Authorization": "Bearer <Access-Token>",
    "Content-Type": "application/json"
}

# List all buckets in the project
buckets_url = f"https://www.googleapis.com/storage/v1/b?project={project_name}"
response = requests.get(buckets_url, headers=headers)
buckets_data = json.loads(response.text)

# Check IAM policy for each bucket
for bucket in buckets_data.get("items", []):
    bucket_name = bucket.get("name")
    iam_url = f"https://www.googleapis.com/storage/v1/b/{bucket_name}/iam"
    response = requests.get(iam_url, headers=headers)
    iam_data = json.loads(response.text)

    # Check if the IAM policy contains allUsers or allAuthenticatedUsers as a member
    for role in iam_data.get("bindings", []):
        if "allUsers" in role.get("members", []) or "allAuthenticatedUsers" in role.get("members", []):
            print(f"Bucket {bucket_name} has public or anonymous access.")
            break
    else:
        print(f"Bucket {bucket_name} is not publicly accessible.")
        
        

        
-------------------------------------------------------------------------------------------------------------------------
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
To use this script, replace <ProjectName> with the name of your project, and <Access-Token> with a valid access token that has the necessary permissions to access the Google Cloud Storage API. The script then uses the REST API to list all the buckets in the project, and for each bucket, it checks its IAM policy to ensure that it does not contain allUsers or allAuthenticatedUsers as a member. If a bucket is found to be publicly accessible, the script prints a message indicating that it has public or anonymous access, otherwise it prints a message indicating that it is not publicly accessible.

Note that to use the Google Cloud Storage REST API, you need to have the necessary permissions and credentials set up. You can find more information about the authentication and authorization requirements for the REST API in the Google Cloud Storage API documentation.


#-----------------------------------------------------------------------


from google.cloud import dns

def check_cloud_dns_logging(project_id):
    client = dns.DnsServiceClient()

    # List all VPC networks in the project
    networks = client.list_networks(project=f"projects/{project_id}")

    for network in networks:
        # Get the VPC network name
        network_name = network.name.split("/")[-1]

        # Get the Cloud DNS logging configuration for the network
        try:
            response = client.get_dns_logging_config(
                request={
                    "name": f"projects/{project_id}/global/networks/{network_name}"
                }
            )
            logging_enabled = response.log_config.enable_log_delivery
        except Exception as e:
            print(f"Error getting DNS logging config for {network_name}: {e}")
            continue

        # Check if logging is enabled for the network
        if not logging_enabled:
            print(f"DNS logging is not enabled for {network_name}!")
            # Implement code to enable logging for the network
        else:
            print(f"DNS logging is enabled for {network_name}.")



#----------------------------------------------------
def check_instances_for_external_ip(project_id):
    # Run the 'gcloud compute instances list' command and capture the output as a byte string
    instances_output = subprocess.check_output(['gcloud', 'compute', 'instances', 'list', '--format=json', f'--project={project_id}'])
    
    # Load the JSON-formatted output into a list of dictionaries
    instances = json.loads(instances_output)

    # Check whether each instance has an external IP address configured
    for instance in instances:
        for interface in instance['networkInterfaces']:
            if 'accessConfigs' in interface:
                print(f"Instance {instance['name']} has an external IP address configured.")
                break
        else:
            print(f"Instance {instance['name']} does not have an external IP address configured.")

check_instances_for_external_ip("innate-portal-377406")

#______________________________________________________________________________________

import subprocess

def ensure_dns_logging_enabled(project_id):
    # Get a list of all VPC networks in the project
    cmd = f"gcloud compute networks list --project {project_id} --format='value(name)'"
    vpc_networks = subprocess.check_output(cmd, shell=True, text=True).strip().split('\n')
    
    # For each VPC network, check if it is associated with a DNS policy with logging enabled
    for network in vpc_networks:
        cmd = f"gcloud dns policies list --project {project_id} --filter='networks.networkUrl.basename()={network}' --format='value(enableLogging)'"
        logging_enabled = subprocess.check_output(cmd, shell=True, text=True).strip()
        
        # If logging is not enabled for the VPC network, enable it
        if logging_enabled.lower() != 'true':
            cmd = f"gcloud dns policies update --project {project_id} --network {network} --enable-logging"
            subprocess.check_call(cmd, shell=True)
            print(f"Enabled DNS logging for VPC network {network}")
